# app.py
# -*- coding: utf-8 -*-
import os
import base64
import shutil
import streamlit as st
import pandas as pd
import requests
from requests.adapters import HTTPAdapter, Retry
import plotly.graph_objs as go
from plotly.subplots import make_subplots
import ta
from datetime import datetime, timedelta
from pytz import timezone
import numpy as np
from typing import Optional, Set

# -----------------------------
# 페이지/스타일
# -----------------------------
st.set_page_config(page_title="Upbit RSI(13) + Bollinger Band 시뮬레이터", layout="wide")
st.markdown("""
<style>
  .block-container {padding-top: 0.8rem; padding-bottom: 0.8rem; max-width: 1100px;}
  .stMetric {text-align:center;}
  .section-title {font-size:1.05rem; font-weight:700; margin: 0.6rem 0 0.2rem;}
  .hint {color:#6b7280;}
  .success-cell {background-color:#FFF59D; color:#E53935; font-weight:600;}
  .fail-cell {color:#1E40AF; font-weight:600;}
  .neutral-cell {color:#FF9800; font-weight:600;}
  table {border-collapse:collapse; width:100%;}
  th, td {border:1px solid #ddd; padding:6px; text-align:center;}
</style>
""", unsafe_allow_html=True)

# -----------------------------
# 업비트 마켓 로드
# -----------------------------
@st.cache_data(ttl=3600)
def get_upbit_krw_markets():
    url = "https://api.upbit.com/v1/market/all"
    try:
        r = requests.get(url, params={"isDetails": "false"}, timeout=8)
        r.raise_for_status()
        items = r.json()
        rows = []
        for it in items:
            mk = it.get("market", "")
            if mk.startswith("KRW-"):
                sym = mk[4:]
                label = f'{it.get("korean_name","")} ({sym}) — {mk}'
                rows.append((label, mk))
        rows.sort(key=lambda x: (x[1] != "KRW-BTC", x[1]))
        if rows:
            return rows
    except Exception:
        pass
    return [("비트코인 (BTC) — KRW-BTC", "KRW-BTC")]

MARKET_LIST = get_upbit_krw_markets()
default_idx = next((i for i, (_, code) in enumerate(MARKET_LIST) if code == "KRW-BTC"), 0)

# -----------------------------
# 타임프레임
# -----------------------------
TF_MAP = {
    "1분": ("minutes/1", 1),
    "3분": ("minutes/3", 3),
    "5분": ("minutes/5", 5),
    "15분": ("minutes/15", 15),
    "30분": ("minutes/30", 30),
    "60분": ("minutes/60", 60),
    "일봉": ("days", 24 * 60),
}

# -----------------------------
# 신호 중복 처리
# -----------------------------
dup_mode = st.radio(
    "신호 중복 처리",
    options=["중복 제거 (연속 동일 결과 1개)", "중복 포함 (연속 신호 모두)"],
    index=0,
    horizontal=True
)

# -----------------------------
# 기본 설정
# -----------------------------
st.markdown('<div class="section-title">① 기본 설정</div>', unsafe_allow_html=True)
c1, c2, c3, c4 = st.columns(4)
with c1:
    market_label, market_code = st.selectbox("종목 선택", MARKET_LIST, index=default_idx, format_func=lambda x: x[0])
with c2:
    tf_label = st.selectbox("봉 종류 선택", list(TF_MAP.keys()), index=2)
with c3:
    KST = timezone("Asia/Seoul")
    today_kst = datetime.now(KST).date()
    default_start = today_kst - timedelta(days=1)
    start_date = st.date_input("시작 날짜", value=default_start)
with c4:
    end_date = st.date_input("종료 날짜", value=today_kst)

interval_key, minutes_per_bar = TF_MAP[tf_label]
st.markdown("---")

# -----------------------------
# 데이터 수집/지표/시뮬레이션
# -----------------------------
_session = requests.Session()
_retries = Retry(total=3, backoff_factor=0.5, status_forcelist=[429, 500, 502, 503, 504])
_session.mount("https://", HTTPAdapter(max_retries=_retries))

def fetch_upbit_paged(market_code, interval_key, start_dt, end_dt, minutes_per_bar, warmup_bars: int = 0):
    """Upbit 캔들 페이징 수집 (CSV 저장/보충 포함 + GitHub 커밋 지원)."""
    import tempfile, shutil

    if "minutes/" in interval_key:
        unit = interval_key.split("/")[1]
        url = f"https://api.upbit.com/v1/candles/minutes/{unit}"
        tf_key = f"{unit}min"
    else:
        url = "https://api.upbit.com/v1/candles/days"
        tf_key = "day"

    data_dir = os.path.join(os.path.dirname(__file__), "data_cache")
    os.makedirs(data_dir, exist_ok=True)
    csv_path = os.path.join(data_dir, f"{market_code}_{tf_key}.csv")

    if os.path.exists(csv_path):
        df_cache = pd.read_csv(csv_path, parse_dates=["time"])
    else:
        root_csv = os.path.join(os.path.dirname(__file__), f"{market_code}_{tf_key}.csv")
        if os.path.exists(root_csv):
            df_cache = pd.read_csv(root_csv, parse_dates=["time"])
        else:
            df_cache = pd.DataFrame(columns=["time","open","high","low","close","volume"])

    # ✅ UTC → KST 보정
    if not df_cache.empty:
        try:
            if df_cache["time"].dt.tz is None:
                df_cache["time"] = df_cache["time"].dt.tz_localize("UTC").dt.tz_convert("Asia/Seoul").dt.tz_localize(None)
        except Exception:
            pass

    if not df_cache.empty:
        cache_min, cache_max = df_cache["time"].min(), df_cache["time"].max()
        if cache_min <= start_dt and cache_max >= end_dt:
            return df_cache[(df_cache["time"] >= start_dt) & (df_cache["time"] <= end_dt)].reset_index(drop=True)

    df_all = df_cache.copy()
    all_new_rows = []

    def _fetch_until(to_time, stop_when):
        got = []
        try:
            while True:
                params = {"market": market_code, "count": 200, "to": to_time.strftime("%Y-%m-%d %H:%M:%S")}
                r = _session.get(url, params=params, headers={"Accept": "application/json"}, timeout=10)
                r.raise_for_status()
                batch = r.json()
                if not batch:
                    break
                got.extend(batch)
                last_ts = pd.to_datetime(batch[-1]["candle_date_time_kst"])
                if stop_when(last_ts):
                    break
                to_time = last_ts - timedelta(seconds=1)
        except Exception:
            pass
        return got

    if not df_cache.empty:
        cache_min, cache_max = df_cache["time"].min(), df_cache["time"].max()
        if cache_max < end_dt:
            tail_rows = _fetch_until(end_dt, stop_when=lambda ts: ts <= cache_max)
            all_new_rows.extend(tail_rows)
        if cache_min > start_dt:
            head_rows = _fetch_until(cache_min, stop_when=lambda ts: ts <= start_dt)
            all_new_rows.extend(head_rows)
    else:
        base_rows = _fetch_until(end_dt, stop_when=lambda ts: ts <= start_dt)
        all_new_rows.extend(base_rows)

    if all_new_rows:
        df_new = pd.DataFrame(all_new_rows).rename(columns={
            "candle_date_time_kst": "time",
            "opening_price": "open",
            "high_price": "high",
            "low_price": "low",
            "trade_price": "close",
            "candle_acc_trade_volume": "volume",
        })
        df_new["time"] = pd.to_datetime(df_new["time"])
        df_new = df_new[["time", "open", "high", "low", "close", "volume"]]

        df_all = pd.concat([df_all, df_new], ignore_index=True)
        df_all = df_all.drop_duplicates(subset=["time"]).sort_values("time").reset_index(drop=True)

        tmp_path = csv_path + ".tmp"
        df_all.to_csv(tmp_path, index=False)
        shutil.move(tmp_path, csv_path)

        ok, msg = github_commit_csv(csv_path)
        if not ok:
            st.warning(f"캔들 CSV는 로컬에 저장됐지만 GitHub 반영 실패: {msg}")

    # ✅ 최종 반환 (여유 범위 ±1일 → 이후 다시 슬라이스)
    df_all = df_all[(df_all["time"] >= start_dt - timedelta(days=1)) & (df_all["time"] <= end_dt + timedelta(days=1))].reset_index(drop=True)
    return df_all[(df_all["time"] >= start_dt) & (df_all["time"] <= end_dt)].reset_index(drop=True)

# -----------------------------
# 이후 add_indicators, simulate, 실행부 등은 기존 코드 그대로
# -----------------------------
try:
    # ... (원래 실행부 전체 유지)
    pass
except Exception as e:
    st.error(f"오류: {e}")
